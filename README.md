# Machine Learning Practice

I'm teaching myself the basics behind machine learning. This repo will be the dumping ground for my notes, toy projects, and things I do while following tutorials. It won't contain anything really original, but I'll have it on my github in case it helps any other learners. 

<img src="./giphy.gif" width="250">

## Table of Contents

1. [MNIST Digits](https://github.com/amandagrice/machine-learning-practice/tree/master/MNIST%20Digits) - A neural net that can read handwritten numbers. 
2. [IMDB Classification](https://github.com/amandagrice/machine-learning-practice/tree/master/IMDB%20Classification) - A neural net that classifies movie reviews from [IMDB](https://www.imdb.com/) as positive or negative.
3. [Reuters Classification](https://github.com/amandagrice/machine-learning-practice/tree/master/Reuters%20Classification) - A neural net that can label Reuters newswires with one of 46 topics. 
4. [Boston Housing Price Predictor](https://github.com/amandagrice/machine-learning-practice/tree/master/Boston%20Housing%20Prices%2070s) - A neural net that predicts housing prices for Boston suburbs from 70s data.

---

## Notes (in progress)

- **artificial intelligence** - computers solving problems generally associated with the human mind

- **machine learning** - the study of algorithms and statistical models that computers use to perform a task without specific instructions but relying on statistics and inference instead. Subset of the field of artificial intelligence. 
  - classical programming: data + rules = answers
  - machine learning: data + answers = rules

- **deep learning** - subset of machine learning. The “deep” part refers to the idea of successive layers of representations. Through machine learning, computers “learn” to represent data throw multiple stages - progressively extracting higher level features from raw input. For example, a deep learning algorithm that takes in images might first recognize edges or colors in lower layers, and then later identify objects in the images. 

- **shallow learning** - other methods of machine learning that don’t rely on layers
  - classical machine learning approaches (not deep learning):
    - probabilistic modeling - application of principles of statistics to data analysis
      - Naive Bayes
      - logistic regression (logreg)
    - kernel methods
      - support vector machine (SVM)
        - decision boundaries
        - separation hyperplane
        - maximizing the margin
      - kernel trick
      - kernel function
    - decision trees
      - Random Forest
      - gradient boosting machines

- **symbolic AI** - main focus of AI from 1950s to 1980s. Researchers tried to represent problems using a series of human-readable rules. 
  - **expert systems** - form of symbolic AI; a computer system that emulates the decision-making ability of a human expert. Generally, tons of if-then code.

- **analytical engine** - a general-purpose mechanical computer designed by Charles Babbage. Never finished. 

- **Turing test** - a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human. Proposed by Alan Turing in 1950. Basically a human evaluator has a conversation with two entities over a text chat - one robot and one human and if the evaluator can’t reliably determine who is the robot - the robot passes.

- hypothesis space
- neural networks
  - convolutional neural networks (convnets)
- weights
  - the transformation implemented by a layer of a NN is parameterized by its weights
  - learning is finding the correct values for weights
- loss function (aka objective function)
- optimizer
- backpropagation
- training loop
- feature engineering
- Kaggle - https://www.kaggle.com/
- Tensor
- Tensor Processing Unit (TPU)
- CPU
- GPU
- Moore’s Law
- regression
- feature-wise normalization
- scalar regression
- linear layouts
- MSE (Mean Squared Error)
- MAE (Mean Absolute Error)
- variance
- K-Fold Cross Validation
- epochs


- **Supervised learning** - goal is to learn the relationship / mapping between training data inputs and known training targets (also called **annotations**)
    - Binary classification
    - Multiclass classification
    - Scalar regression
- **Unsupervised learning** - find interesting transformations of the input data without the help of any targets
    - Used for data visualizations, data compression, data denoising, or find correlations
    - Dimensionality reduction
    - Clustering
- **Self-Supervised Learning** - Supervised learning without human-annotated labels. 
    - Labels are often generated by the input data.
    - ex. Autoencoders - targets are the input, unmodified
    - ex. Temporally supervised learning - ex. Trying to predict next frame in a video
- **Reinforcement Learning** - An agent receives information about its environment and tries to maximize for some reward. 
    - Ex. Google DeepMind learning to play Atari games or Go
- **Sample** or **input** -- one data point that goes into your model
- **Prediction** or **output** -- what comes out of your model
- **Target** -- the truth. What your model should ideally have predicted.
- **Prediction error** or **loss value** -- a measure of the distance between your prediction and the target
- **Classes** - a set of possible labels to choose from in a classification problem
- **Label** - an instance of a class annotation in a classification problem. For example, a digit image being labelled 9. 
- **Ground-truth** or **annotations** -- all targets for a dataset, typically collected by humans. 
- **Binary classification** -- a classification task where you’re classifying into 2 categories
- **Multiclass classification** --- a classification task with more than 2 categories
- **Multilabel classification** -- a classification task where input can be assigned multiple labels. For example, an image having both a cat and a dog in it should be labelled with “cat” and “dog.”
- **Scalar regression** -- a task where the target is a continuous scalar value. Ex. any number, not just 0 or 1
- **Vector regression** -- a task where the target is a set of continuous values.
- **Mini-batch** or **batch** -- a small set of samples that are processed simultaneously by a model. 
- **Overfitting** -- when a model starts performing worse because it has been trained too much on too narrow a set of data
- In machine learning, the goal is to achieve models that **generalize** -- that perform well on never-before-seen data.
- **Training, validation, & test sets** -- To evaluate a model, you split the available data into 3 sets. You train on the training data, evaluate on the validation data. Once your model is ready you finally test it on the test data. 
- **Hyperparameters** -- # of layers, size of the layers
- **Parameters** -- the network’s weights
- **Information leaks** -- whenever you tune a model hyperparameter based on the model’s performance, some info about the validation data leaks into the model. This is why you leave a 3rd set untouched until the end -- to insure your model is truly able to generalize and hasn’t just learned the validation data.
- **Simple hold-out validation** -- simply dividing the data into some fraction of training, validation, and test data. Works well for large enough datasets, but if the dataset is too small, the validation set can be too small to be statistically significant - leading to wildly different results each time. 
- **K-Fold Validation**
    1. Shuffle the datase
    2. Split the data set into k groups
    3. For each group:
        1. Make the group a test data set
        2. Make the remaining groups the training data set
        3. Fit a model on the training set and evaluate it on the test set
        4. Keep the evaluation score and discard the model
    4. Average the evaluation scores.
    
- **Iterated K-Fold Validation with Shuffling** -- running K-Fold Validation multiple times, shuffling the data every time before splitting into K groups. The final score is the average of the scores obtained at each run of the K-fold validation. You end up training and evaluating P * K models (where P is the number of iterations). This can end up expensive.
- **Data preprocessing** aims at making the raw data better for neural networks to learn from. Forms of preprocessing include:
    - **Vectorization** - turning data to tensors
    - **Normalization** - change data to use a common scale. Generally small, homogenous values. Ex. turn everything into a 0-1 range with one standard deviation.
    - **Handling missing values**
    - **Feature Extraction / Feature Engineering** -- make a problem easier by expressing it in a simpler way. Requires a good understanding of the problem in depth. This was far more necessary with shallow learning. 




